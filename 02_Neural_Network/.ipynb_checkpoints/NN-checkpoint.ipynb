{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.095325Z",
     "start_time": "2020-11-08T18:04:16.047351Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.127208Z",
     "start_time": "2020-11-08T18:04:16.100751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store user specified arguments into variables\n",
    "train_file = 'train_data4.txt'\n",
    "target_file = 'train_target4.txt'\n",
    "layer_num = 2\n",
    "units = [4,1]\n",
    "activation = 'sigmoid' #'tanh'\n",
    "loss = 'SSE' #'CE'\n",
    "learn_rate = 0.1\n",
    "max_epochs = 1E4 #1E7\n",
    "batch_size = 10\n",
    "tolerance = 0.05\n",
    "output_file = 'output_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.165691Z",
     "start_time": "2020-11-08T18:04:16.137149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load feature matrix (training inputs) and target vector (regression targets)\n",
    "feature = np.genfromtxt(train_file, delimiter=' ')\n",
    "\n",
    "target = np.genfromtxt(target_file, delimiter=' ')\n",
    "target = target[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T19:49:54.956843Z",
     "start_time": "2020-11-08T19:49:54.951072Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 5)\n",
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the dimension of the training feature and target\n",
    "print(feature.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.269333Z",
     "start_time": "2020-11-08T18:04:16.248379Z"
    }
   },
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"\n",
    "\n",
    "    A class for activation functions and their derivatives\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    y : (len(x), ) ndarray of float\n",
    "        the output from the activiation function\n",
    "\n",
    "    y_prime : (len(x), ) ndarray of float\n",
    "        the derivative the activiation function\n",
    "\n",
    "    activation : str\n",
    "        the name of the activation function specified by the user ('sigmoid' or 'tanh')\n",
    "\n",
    "    scale_x : float\n",
    "        a factor that will be multiplied with input x (default 1)\n",
    "\n",
    "    scale_y : float\n",
    "        a factor that will be multiplied with result y before returning (default 1)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    sigmoid(x)\n",
    "        evaluate the sigmoid of input x\n",
    "\n",
    "    sigmoid_derivative()\n",
    "        evaluate the derivative of the sigmoid given x\n",
    "\n",
    "    tanh(x)\n",
    "        evaluate the tanh of input x\n",
    "\n",
    "    tanh_derivative()\n",
    "        evaluate the derivative of the tanh given x\n",
    "\n",
    "    call(x)\n",
    "        a driver function to evaluate the given activiation function\n",
    "\n",
    "    differentiate()\n",
    "        a driver function to evaluate the derivative of the given activiation function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation):\n",
    "        \"\"\"\n",
    "\n",
    "        Initialize all attributes of the class\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        activation : str\n",
    "            the name of the activation function specified by the user ('sigmoid' or 'tanh')\n",
    "\n",
    "        \"\"\"\n",
    "        self.y = []\n",
    "        self.y_prime = []\n",
    "        self.activation = activation\n",
    "        self.scale_x = 1\n",
    "        self.scale_y = 1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Evaluate the sigmoid of input x\n",
    "\n",
    "        y = 1 / (1 + exp(-x * scale_x))\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        x : (len(x),) ndarray of float\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (len(x), ) ndarray of float\n",
    "            output from the sigmoid function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.y = 1 / (1 + np.exp(-x * self.scale_x))\n",
    "\n",
    "        return self.y\n",
    "\n",
    "    def sigmoid_derivative(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Evaluate the derivative of the sigmoid given x\n",
    "\n",
    "        Given y,\n",
    "        y' = y * (1 - y) * scale_x\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_prime : (len(x), ) ndarray of float\n",
    "            output from differentiating the sigmoid function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.y_prime = self.y * (1 - self.y) * self.scale_x\n",
    "\n",
    "        return self.y_prime\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Evaluate the tanh of input x\n",
    "\n",
    "        y =  tanh(x_scale * x) * y_scale\n",
    "        tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (len(x), ) ndarray of float\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (len(x), ) ndarray of float\n",
    "            output from the tanh function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.y = np.tanh(x * self.scale_x) * self.scale_y\n",
    "\n",
    "        return self.y\n",
    "\n",
    "    def tanh_derivative(self):\n",
    "        \"\"\"\n",
    "\n",
    "        Evaluate the derivative of the tanh given x\n",
    "\n",
    "        Given y,\n",
    "        y' = (scale_y - y) * (scale_y + y) * (scale_x / scale_y) \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_prime : (len(x), ) ndarray of float\n",
    "            output from differentiating the tanh function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.y_prime = (self.scale_y - self.y) * \\\n",
    "            (self.scale_y + self.y) * (self.scale_x / self.scale_y)\n",
    "\n",
    "        return self.y_prime\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        A driver function to evaluate the given activiation function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (len(x), ) ndarray of float\n",
    "            input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (len(x), ) ndarray of float\n",
    "            output from the given activiation function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "        elif self.activation == 'tanh':\n",
    "\n",
    "            return self.tanh(x)\n",
    "\n",
    "    def differentiate(self):\n",
    "        \"\"\"\n",
    "\n",
    "        A driver function to evaluate the derivative of the given activiation function\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (len(x), ) ndarray of float\n",
    "            output from the given activiation function\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == 'sigmoid':\n",
    "\n",
    "            return self.sigmoid_derivative()\n",
    "\n",
    "        elif self.activation == 'tanh':\n",
    "\n",
    "            return self.tanh_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.291985Z",
     "start_time": "2020-11-08T18:04:16.276476Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "\n",
    "    A class containing the data structure for a dense layer, and some helper functions\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_output : int\n",
    "        number of neurons in the layer\n",
    "\n",
    "    activation : Activation\n",
    "        a class for evaluating the activation function and its derivative\n",
    "\n",
    "    weight : (n_input + 1, n_output) ndarray of float\n",
    "        weight matrix of the layer, accounting for a bias term\n",
    "\n",
    "    prev_wght_update : (n_input + 1, n_output) ndarray of float\n",
    "        record the previous weight matrix\n",
    "\n",
    "    input : (n_input + 1, ) ndarray of float\n",
    "        input to the layer with an additional bias term, [x_1, ... , x_n, 1]\n",
    "\n",
    "    output : (n_output, ) ndarray of float\n",
    "        output the layer, [y_1, ... , y_m]\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    build(n_input)\n",
    "        initialize weight, prev_wght_update, and output with appropriate dimension;\n",
    "        initialize weight matrix with random numbers from -1 to 1\n",
    "\n",
    "    call(x)\n",
    "        compute the output of the layer given input vector x\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_output_nodes, activation):\n",
    "        \"\"\"\n",
    "\n",
    "        Initialize the layer by specifying the number of neurons and the activation function\n",
    "\n",
    "        Parameters\n",
    "        ---------- \n",
    "        n_output_nodes : int\n",
    "            number of neurons in the layer\n",
    "\n",
    "        activation : str\n",
    "            the activiation function used in the layer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_output = n_output_nodes\n",
    "        self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, n_input):\n",
    "        \"\"\"\n",
    "\n",
    "        Initialize weight, prev_wght_update, and output with appropriate dimension;\n",
    "        Initialize weight matrix with random numbers from -1 to 1\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_input : int\n",
    "            number of input data (dimension of the features or number of neurons in the previous layer)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.weight = np.random.uniform(\n",
    "            low=-1, high=1, size=(n_input + 1, self.n_output))\n",
    "        self.output = np.zeros((1, self.n_output))\n",
    "        self.prev_wght_update = np.zeros((n_input + 1, self.n_output))\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        Compute the output of the layer given input vector x\n",
    "        \n",
    "        y = activation_function(x @ weight)\n",
    "        (n_output, ) = activation_function((n_input + 1, ) @ (n_input + 1, n_output))\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (n_input, ) ndarray of float\n",
    "            input to the layer, [x_1, ... , x_n]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output : (n_output, ) ndarray of float\n",
    "            output the layer, [y_1, ... , y_m]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = np.append(x, 1)\n",
    "        v = np.matmul(self.input, self.weight)\n",
    "        self.output = self.activation.call(v)\n",
    "\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.310640Z",
     "start_time": "2020-11-08T18:04:16.294769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87265074, 0.53823522, 0.26966516, 0.57152884])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "np.random.seed(2020)\n",
    "layer = Dense(4, activation='sigmoid')\n",
    "layer.build(feature.shape[1])\n",
    "layer.call(feature[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.330619Z",
     "start_time": "2020-11-08T18:04:16.316683Z"
    }
   },
   "outputs": [],
   "source": [
    "class Losses:\n",
    "    \"\"\"\n",
    "\n",
    "    A class for loss functions and their gradients\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    loss : str\n",
    "        the name of the loss function specified by the user ('SSE' or 'CE')\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    squared_error(y_true, y_pred)\n",
    "        evaluate the squared error loss\n",
    "\n",
    "    cross_entropy(y_true, y_pred)\n",
    "        evaluate the cross entropy loss\n",
    "\n",
    "    squared_error_grad(y_true, y_pred)\n",
    "        evaluate the tanh of input x\n",
    "\n",
    "    cross_entropy_grad(y_true, y_pred)\n",
    "        evaluate the derivative of the tanh given x\n",
    "\n",
    "    __call__(y_true, y_pred)\n",
    "        a driver function to evaluate the given activiation function\n",
    "\n",
    "    gradient(y_true, y_pred)\n",
    "        a driver function to evaluate the derivative of the given activiation function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        self.loss = loss\n",
    "        \n",
    "    def squared_error(self, y_true, y_pred):\n",
    "        return (y_true - y_pred) ** 2\n",
    "    \n",
    "    def cross_entropy(self, y_true, y_pred):\n",
    "        return - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "    \n",
    "    def squared_error_grad(self, y_true, y_pred):\n",
    "        return y_true - y_pred\n",
    "    \n",
    "    def cross_entropy_grad(self, y_true, y_pred):\n",
    "        return (y_true - y_pred) / (y_pred - y_pred ** 2)\n",
    "    \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        if self.loss == 'SSE':\n",
    "            return self.squared_error(y_true, y_pred)\n",
    "        elif self.loss == 'CE':\n",
    "            return self.cross_entropy(y_true, y_pred)\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        if self.loss == 'SSE':\n",
    "            return self.squared_error_grad(y_true, y_pred)\n",
    "        elif self.loss == 'CE':\n",
    "            return self.cross_entropy_grad(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T19:48:59.471660Z",
     "start_time": "2020-11-08T19:48:59.459229Z"
    }
   },
   "outputs": [],
   "source": [
    "class Multilayers:\n",
    "    def __init__(self, x_shape, layers, input_scale=1, output_scale=1):\n",
    "        self.model = layers\n",
    "        self.n_layers = len(layers)\n",
    "\n",
    "        n_input = x_shape\n",
    "        for layer in self.model:\n",
    "            layer.build(n_input)\n",
    "            layer.activation.x_scale = input_scale\n",
    "            layer.activation.y_scale = output_scale\n",
    "            n_input = layer.n_output\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        input_signal = x\n",
    "        for i, layer in zip(range(self.n_layers), self.model):\n",
    "            output_signal = layer.call(input_signal)\n",
    "            \n",
    "            input_signal = output_signal\n",
    "\n",
    "        self.y = output_signal\n",
    "        return self.y\n",
    "\n",
    "    def back_prop(self, target, learn_rate, loss, momentum=0):\n",
    "        prev_update = 0\n",
    "        \n",
    "        i = 0\n",
    "        for layer in reversed(self.model):\n",
    "                           \n",
    "            prime = layer.activation.differentiate()\n",
    "            \n",
    "            if i == 0:\n",
    "                \n",
    "                errors = loss.gradient(target, self.y)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                errors = np.inner(deltas, prev_weight[:-1])\n",
    "                \n",
    "            deltas = errors * prime\n",
    "            \n",
    "            prev_weight = np.copy(layer.weight)\n",
    "            \n",
    "            update = learn_rate * np.outer(layer.input, deltas)\n",
    "            \n",
    "            layer.weight += momentum * layer.prev_wght_update + update\n",
    "            \n",
    "            layer.prev_wght_update = update\n",
    "            \n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T19:49:00.445404Z",
     "start_time": "2020-11-08T19:49:00.433328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before back prop: [0.36477282 0.17336076]\n",
      "(2,) (2,) (2,)\n",
      "(3,) (3,) (3,)\n",
      "(5,) (5,) (5,)\n",
      "After 1 back prop: [0.27658303 0.12676493]\n",
      "(2,) (2,) (2,)\n",
      "(3,) (3,) (3,)\n",
      "(5,) (5,) (5,)\n",
      "After 2 back prop: [0.20351656 0.09080108]\n",
      "(2,) (2,) (2,)\n",
      "(3,) (3,) (3,)\n",
      "(5,) (5,) (5,)\n",
      "After 3 back prop: [0.14542603 0.06356227]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2020)\n",
    "\n",
    "model = Multilayers(feature.shape[1],[\n",
    "    Dense(5, activation='sigmoid'),\n",
    "    Dense(3, activation='sigmoid'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print('Before back prop:', model.predict(feature[0]))\n",
    "model.back_prop(target[0], learn_rate=0.1, loss=loss_function)\n",
    "print('After 1 back prop:', model.predict(feature[0]))\n",
    "model.back_prop(target[0], learn_rate=0.1, loss=loss_function)\n",
    "print('After 2 back prop:', model.predict(feature[0]))\n",
    "model.back_prop(target[0], learn_rate=0.1, loss=loss_function)\n",
    "print('After 3 back prop:', model.predict(feature[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.496086Z",
     "start_time": "2020-11-08T18:04:16.465222Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, feature, target, loss='SSE', learn_rate=0.1, batch_size=9, momentum=0):\n",
    "    epoch_size = len(feature)\n",
    "    batch_num = int(np.ceil(epoch_size / batch_size))\n",
    "\n",
    "    order = np.random.choice(range(epoch_size), size=epoch_size, replace=False)\n",
    "    loss_function = Losses(loss)\n",
    "    history = []\n",
    "\n",
    "    this_batch_size = 0\n",
    "    i = 0\n",
    "    for batch_i in range(batch_num):\n",
    "        if batch_i == batch_num - 1 and epoch_size % batch_size != 0:\n",
    "            this_batch_size = epoch_size % batch_size\n",
    "        else:\n",
    "            this_batch_size = batch_size\n",
    "        \n",
    "        batch_count = 0\n",
    "        batch_error = 0\n",
    "        while batch_count < this_batch_size:\n",
    "            pos = order[i]\n",
    "            y_pred = model.predict(feature[pos])\n",
    "            y_true = target[pos]\n",
    "            model.back_prop(y_true, learn_rate, loss_function, momentum)\n",
    "            \n",
    "            batch_error += np.mean(loss_function(y_true, y_pred))\n",
    "                                   \n",
    "            batch_count += 1\n",
    "            i += 1\n",
    "            \n",
    "        mean_batch_error = batch_error / this_batch_size\n",
    "        history.append(mean_batch_error)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.527861Z",
     "start_time": "2020-11-08T18:04:16.505993Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(label_vector):\n",
    "    binary_rep = ['{0:b}'.format(int(label)) for label in label_vector]\n",
    "    max_bit_num = max([len(binary_label) for binary_label in binary_rep])\n",
    "    binary_rep = [binary_label.zfill(max_bit_num) for binary_label in binary_rep]\n",
    "    binary_rep = [[int(bit) for bit in binary_label] for binary_label in binary_rep]\n",
    "    binary_rep = np.array(binary_rep)\n",
    "    return binary_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.545936Z",
     "start_time": "2020-11-08T18:04:16.531757Z"
    }
   },
   "outputs": [],
   "source": [
    "periodic_output = 2E2\n",
    "plt.rcParams['figure.figsize'] = (16,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:04:16.620138Z",
     "start_time": "2020-11-08T18:04:16.551635Z"
    }
   },
   "outputs": [],
   "source": [
    "file_num = 1\n",
    "\n",
    "# Store user specified arguments into variables\n",
    "train_file = f'train_data{file_num}.txt'\n",
    "target_file = f'train_target{file_num}.txt'\n",
    "\n",
    "feature = np.genfromtxt(train_file, delimiter=' ')\n",
    "target = np.genfromtxt(target_file, delimiter=' ')\n",
    "target = target[:, np.newaxis]\n",
    "\n",
    "layer_num = 3\n",
    "units = [5,3,1]\n",
    "activation = 'tanh' #'tanh' 'sigmoid'\n",
    "loss = 'SSE' #'CE' 'SSE'\n",
    "learn_rate = 0.5 #0.05\n",
    "max_epochs = 5E3 #1E4\n",
    "batch_size = 20\n",
    "tolerance = 1E-4\n",
    "output_file = f'output{file_num}.txt'\n",
    "\n",
    "momentum = 0.1 #0.1\n",
    "\n",
    "if loss == 'CE':\n",
    "    target = one_hot_encoding(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:06:08.176766Z",
     "start_time": "2020-11-08T18:04:16.623476Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: SSE; Learning Rate: 0.5000; Batch Size: 20\n",
      "0 1.6880425096481495\n",
      "1000 1.3973344450422558\n",
      "2000 1.0005286566656406\n",
      "3000 1.5084671260818885\n",
      "4000 1.4628810873395799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-738-37abf67f714a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mmin_batch_error\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmin_batch_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Delete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-734-0d47eca16370>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, feature, target, loss, learn_rate, batch_size, momentum)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mbatch_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-732-2a84dae1df73>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, target, learn_rate, loss, momentum)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mprev_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(a, order)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \"\"\"\n\u001b[0;32m--> 775\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;31m# Basic operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Loss: {loss}; Learning Rate: {learn_rate:.4f}; Batch Size: {batch_size}')\n",
    "#target_range = target.max() - target.min()\n",
    "target_range = 4\n",
    "\n",
    "model = Multilayers(feature.shape[1], \n",
    "                    [Dense(n_output_nodes, activation) for n_output_nodes in units], \n",
    "                    output_scale=target_range,\n",
    "                    input_scale=1)\n",
    "\n",
    "min_batch_error = tolerance + 1\n",
    "error = []\n",
    "history = []\n",
    "\n",
    "start_time = time.time()\n",
    "i = 0\n",
    "while min_batch_error > tolerance and i < max_epochs:\n",
    "    error = train(model, feature, target, loss, learn_rate, batch_size, momentum=momentum)\n",
    "    min_batch_error = min(error)\n",
    "    error = np.mean(error) # Delete\n",
    "    history = np.append(history, error)\n",
    "    \n",
    "    if i % periodic_output == 0:\n",
    "        print(i, min_batch_error)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "print(f\"--- {(time.time() - start_time):.6f}s seconds ---\")\n",
    "print(i, min_batch_error)\n",
    "np.savetxt(output_file, history, delimiter=' ')\n",
    "\n",
    "plt.title(f'Loss: {loss}; Learning Rate: {learn_rate:.2f}; Batch Size: {batch_size}')\n",
    "plt.plot(history, color='red')\n",
    "plt.plot([0, len(history)], [0, 0])\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:06:08.344928Z",
     "start_time": "2020-11-08T18:04:15.991Z"
    }
   },
   "outputs": [],
   "source": [
    "def lookinside(model):\n",
    "    print('------Weights------')\n",
    "    print()\n",
    "    for layer in model.model:\n",
    "        print(layer.weight)\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    print('------Outputs------')\n",
    "    print()\n",
    "    for layer in model.model:\n",
    "        print(layer.output)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:06:08.353087Z",
     "start_time": "2020-11-08T18:04:15.995Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "for i in range(feature.shape[1]):\n",
    "    #plt.hist(feature[:,i])\n",
    "    plt.plot(feature[:,i], target, 'o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T18:06:08.358277Z",
     "start_time": "2020-11-08T18:04:15.998Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transform = Activation(activation)\n",
    "transform.scale_y = 3\n",
    "transform.scale_x = 1\n",
    "x = np.arange(-3,3,0.001)\n",
    "y = transform.call(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
